{
 "metadata": {
  "name": "lsi-poetry"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "A Text Analysis Problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook shows the general process you would use to analyze a corpus of text using sparse vectors. We'll introduce some basics for dealing with text, and we'll look at a few common ideas in language processing. Don't worry if some of the details below are unclear - the goal is to show you the overall process of modeling text in a way that allows us to do machine learning. The purpose of the rest of the course is to dive into the details of what we're doing here so that you can produce similar (and better) analyses on your own.\n",
      "\n",
      "The scenario that we're imagining is that we're interested in the writings of the metaphysical poets. In particular, we're going to see if we can use some really basic natural language processing to compare a poem of Andrew Marvell with some poems by John Donne. We'll see that this doesn't really work very well, but we'll spend the rest of the course looking at ways to make this sort of analysis much better.\n",
      "\n",
      "First, let's import the library that we're going to use: gensim. Gensim provides facilities for modeling text using a few common algorithms. We're only going to scratch the surface today."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These two lines below enable logging, which is how gensim prints some of its information."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import logging\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The two documents are some of John Donne's poems. One is a sonnet addressing Death. The other is a love poem (Elegy XIX). We'll use these as a corpus of documents and query against them using the methods below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents = [\"\"\"Death, be not proud, \n",
      "though some have called thee Mighty and dreadful, \n",
      "for thou are not so; \n",
      "For those whom thou think'st thou dost overthrow Die not, \n",
      "poor Death, nor yet canst thou kill me. \n",
      "From rest and sleep, which but thy pictures be, \n",
      "Much pleasure; then from thee much more must flow, \n",
      "And soonest our best men with thee do go, \n",
      "Rest of their bones, and soul's delivery. \n",
      "Thou'art slave to fate, chance, kings, and desperate men, \n",
      "And dost with poison, war, and sickness dwell, \n",
      "And poppy'or charms can make us sleep as well \n",
      "And better than thy stroke; why swell'st thou then? \n",
      "One short sleep past, we wake eternally,\n",
      "And death shall be no more; Death, thou shalt die.\"\"\",\n",
      "             \n",
      "\"\"\"Come, madam, come, all rest my powers defy, \n",
      "Until I labour, I in labour lie. \n",
      "The foe oft-times having the foe in sight, \n",
      "Is tired with standing though he never fight. \n",
      "Off with that girdle, like heaven's zone glistering, \n",
      "But a far fairer world encompassing. \n",
      "Unpin that spangled breastplate which you wear, \n",
      "That th'eyes of busy fools may be stopped there. \n",
      "Unlace yourself, for that harmonious chime \n",
      "Tells me from you that now it is bed time. \n",
      "Off with that happy busk, which I envy, \n",
      "That still can be, and still can stand so nigh. \n",
      "Your gown going off, such beauteous state reveals, \n",
      "As when from flowery meads th'hills shadow steals. \n",
      "Off with your wiry coronet and show \n",
      "The hairy diadem which on you doth grow: \n",
      "Now off with those shoes: and then safely tread \n",
      "In this love's hallowed temple, this soft bed. \n",
      "In such white robes heaven's angels used to be \n",
      "Received by men; thou, Angel, bring'st with thee \n",
      "A heaven like Mahomet's Paradise; and though \n",
      "Ill spirits walk in white, we easily know \n",
      "By this these Angels from an evil sprite: \n",
      "Those set our hairs, but these our flesh upright. \n",
      "License my roving hands, and let them go \n",
      "Before, behind, between, above, below. \n",
      "O my America! my new-found-land, \n",
      "My kingdom, safeliest when with one man manned, \n",
      "My mine of precious stones, my empery, \n",
      "How blest am I in this discovering thee! \n",
      "To enter in these bonds is to be free; \n",
      "Then where my hand is set, my seal shall be. \n",
      "Full nakedness! All joys are due to thee, \n",
      "As souls unbodied, bodies unclothed must be, \n",
      "To taste whole joys. Gems which you women use \n",
      "Are as Atlanta's balls, cast in men's views, \n",
      "That when a fool's eye lighteth on a gem, \n",
      "His earthly soul may covet theirs, not them: \n",
      "Like pictures, or like books' gay coverings made \n",
      "For lay-men, are all women thus arrayed. \n",
      "Themselves are mystic books, which only we \n",
      "(Whom their imputed grace will dignify) \n",
      "Must see revealed. Then, since that I may know, \n",
      "As liberally as to a midwife, show \n",
      "Thyself: cast all, yea, this white linen hence, \n",
      "There is no penance due to innocence: \n",
      "To teach thee, I am naked first; why than, \n",
      "What need'st thou have more covering than a man?\"\"\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A \"stop list\" is a list of words you want to exclude from consideration in your system. Typically these are words that are so common that they are not informative at all.\n",
      "\n",
      "We're filtering the documents above using the stop list, and converting them from strings to lists of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stoplist = set('these those we thou thee for a of the and to in with you when all i my that which as this is are from'.split())\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
      "         for document in documents]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print texts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['death,', 'be', 'not', 'proud,', 'though', 'some', 'have', 'called', 'mighty', 'dreadful,', 'not', 'so;', 'whom', \"think'st\", 'dost', 'overthrow', 'die', 'not,', 'poor', 'death,', 'nor', 'yet', 'canst', 'kill', 'me.', 'rest', 'sleep,', 'but', 'thy', 'pictures', 'be,', 'much', 'pleasure;', 'then', 'much', 'more', 'must', 'flow,', 'soonest', 'our', 'best', 'men', 'do', 'go,', 'rest', 'their', 'bones,', \"soul's\", 'delivery.', \"thou'art\", 'slave', 'fate,', 'chance,', 'kings,', 'desperate', 'men,', 'dost', 'poison,', 'war,', 'sickness', 'dwell,', \"poppy'or\", 'charms', 'can', 'make', 'us', 'sleep', 'well', 'better', 'than', 'thy', 'stroke;', 'why', \"swell'st\", 'then?', 'one', 'short', 'sleep', 'past,', 'wake', 'eternally,', 'death', 'shall', 'be', 'no', 'more;', 'death,', 'shalt', 'die.'], ['come,', 'madam,', 'come,', 'rest', 'powers', 'defy,', 'until', 'labour,', 'labour', 'lie.', 'foe', 'oft-times', 'having', 'foe', 'sight,', 'tired', 'standing', 'though', 'he', 'never', 'fight.', 'off', 'girdle,', 'like', \"heaven's\", 'zone', 'glistering,', 'but', 'far', 'fairer', 'world', 'encompassing.', 'unpin', 'spangled', 'breastplate', 'wear,', \"th'eyes\", 'busy', 'fools', 'may', 'be', 'stopped', 'there.', 'unlace', 'yourself,', 'harmonious', 'chime', 'tells', 'me', 'now', 'it', 'bed', 'time.', 'off', 'happy', 'busk,', 'envy,', 'still', 'can', 'be,', 'still', 'can', 'stand', 'so', 'nigh.', 'your', 'gown', 'going', 'off,', 'such', 'beauteous', 'state', 'reveals,', 'flowery', 'meads', \"th'hills\", 'shadow', 'steals.', 'off', 'your', 'wiry', 'coronet', 'show', 'hairy', 'diadem', 'on', 'doth', 'grow:', 'now', 'off', 'shoes:', 'then', 'safely', 'tread', \"love's\", 'hallowed', 'temple,', 'soft', 'bed.', 'such', 'white', 'robes', \"heaven's\", 'angels', 'used', 'be', 'received', 'by', 'men;', 'thou,', 'angel,', \"bring'st\", 'heaven', 'like', \"mahomet's\", 'paradise;', 'though', 'ill', 'spirits', 'walk', 'white,', 'easily', 'know', 'by', 'angels', 'an', 'evil', 'sprite:', 'set', 'our', 'hairs,', 'but', 'our', 'flesh', 'upright.', 'license', 'roving', 'hands,', 'let', 'them', 'go', 'before,', 'behind,', 'between,', 'above,', 'below.', 'o', 'america!', 'new-found-land,', 'kingdom,', 'safeliest', 'one', 'man', 'manned,', 'mine', 'precious', 'stones,', 'empery,', 'how', 'blest', 'am', 'discovering', 'thee!', 'enter', 'bonds', 'be', 'free;', 'then', 'where', 'hand', 'set,', 'seal', 'shall', 'be.', 'full', 'nakedness!', 'joys', 'due', 'thee,', 'souls', 'unbodied,', 'bodies', 'unclothed', 'must', 'be,', 'taste', 'whole', 'joys.', 'gems', 'women', 'use', \"atlanta's\", 'balls,', 'cast', \"men's\", 'views,', \"fool's\", 'eye', 'lighteth', 'on', 'gem,', 'his', 'earthly', 'soul', 'may', 'covet', 'theirs,', 'not', 'them:', 'like', 'pictures,', 'or', 'like', \"books'\", 'gay', 'coverings', 'made', 'lay-men,', 'women', 'thus', 'arrayed.', 'themselves', 'mystic', 'books,', 'only', '(whom', 'their', 'imputed', 'grace', 'will', 'dignify)', 'must', 'see', 'revealed.', 'then,', 'since', 'may', 'know,', 'liberally', 'midwife,', 'show', 'thyself:', 'cast', 'all,', 'yea,', 'white', 'linen', 'hence,', 'there', 'no', 'penance', 'due', 'innocence:', 'teach', 'thee,', 'am', 'naked', 'first;', 'why', 'than,', 'what', \"need'st\", 'have', 'more', 'covering', 'than', 'man?']]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Uncomment the code in the cell below if you want to see how the system behaves when restricted to words that appear more than once..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import operator\n",
      "#all_tokens = reduce(operator.add, texts, [])\n",
      "#tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
      "#texts = [[word for word in text if word not in tokens_once] for text in texts]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A \"dictionary\" (in the gensim sense) implements the mapping from words to integer IDs. Here we create a dictionary from the texts array, serialize it to disk, and print a summary of the object:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary = corpora.Dictionary(texts)\n",
      "dictionary.save('/tmp/johndonne.txt')\n",
      "print dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dictionary(295 unique tokens)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The token2id function shows us the mapping from words to indices in the sparse vector format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dictionary.token2id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'gown': 148, \"th'hills\": 254, 'since': 233, 'pictures,': 215, 'flesh': 134, 'below.': 97, 'rest': 51, 'sleep': 57, 'world': 290, 'go': 146, 'man?': 189, 'yet': 79, 'dost': 18, 'gems': 143, 'pleasure;': 46, 'death': 11, 'girdle,': 144, 'kingdom,': 171, \"atlanta's\": 89, 'hairs,': 151, 'nigh.': 204, 'delivery.': 13, 'better': 3, 'only': 211, 'going': 147, 'between,': 98, 'then?': 68, 'beauteous': 92, 'overthrow': 43, 'discovering': 119, 'he': 159, 'shoes:': 230, 'bones,': 4, 'theirs,': 258, 'do': 17, 'them': 259, 'his': 163, 'bonds': 101, 'far': 131, 'views,': 279, 'robes': 221, 'than,': 255, 'thee!': 256, 'more;': 34, 'me': 192, \"books'\": 102, 'know': 172, 'chance,': 9, 'not': 39, 'sleep,': 58, 'shadow': 229, 'now': 205, 'roving': 222, 'nor': 38, \"soul's\": 62, 'spirits': 239, 'heaven': 160, 'like': 182, 'covering': 113, 'die': 15, 'all,': 82, 'them:': 260, 'men,': 31, 'books,': 103, 'labour,': 175, 'though': 71, 'blest': 99, 'kill': 26, 'soft': 235, 'kings,': 27, 'gem,': 142, 'still': 245, 'canst': 8, 'penance': 214, 'glistering,': 145, '(whom': 80, 'naked': 199, 'me.': 29, 'some': 60, 'soul': 236, 'revealed.': 219, 'full': 140, 'mighty': 32, 'past,': 44, 'desperate': 14, 'seal': 225, 'spangled': 238, 'our': 42, 'wear,': 281, 'dwell,': 20, 'before,': 95, 'come,': 111, 'time.': 268, \"heaven's\": 161, 'lighteth': 181, 'madam,': 185, 'unlace': 273, 'pictures': 45, 'shall': 52, 'hence,': 162, 'foe': 136, 'state': 243, 'sprite:': 240, 'imputed': 166, 'coronet': 112, 'stopped': 247, 'america!': 84, 'wiry': 288, 'hairy': 152, 'be': 0, 'defy,': 116, 'bed': 93, 'breastplate': 104, 'yea,': 291, 'men': 30, 'use': 277, 'empery,': 124, 'white': 284, 'busy': 107, 'let': 177, 'midwife,': 196, 'temple,': 252, 'eye': 129, 'precious': 217, 'paradise;': 213, 'by': 108, 'liberally': 178, 'free;': 139, 'go,': 24, 'your': 292, 'then,': 262, \"think'st\": 69, 'where': 283, 'ill': 165, 'so': 234, 'o': 206, 'thus': 266, 'yourself,': 293, 'women': 289, 'above,': 81, \"fool's\": 137, 'stand': 241, 'safely': 224, 'innocence:': 167, 'stones,': 246, 'arrayed.': 88, 'there': 263, 'see': 226, 'dignify)': 118, 'one': 41, 'war,': 75, 'set': 227, 'encompassing.': 125, 'chime': 110, 'know,': 173, 'teach': 250, 'standing': 242, 'there.': 264, 'thyself:': 267, 'be.': 91, 'tread': 270, 'be,': 1, 'their': 66, 'lay-men,': 176, 'due': 121, 'hallowed': 153, 'whom': 77, 'much': 35, 'reveals,': 220, 'behind,': 96, 'show': 231, 'license': 179, 'themselves': 261, 'man': 188, 'angels': 87, 'until': 275, \"mahomet's\": 187, 'more': 33, \"love's\": 184, 'safeliest': 223, 'gay': 141, 'on': 210, 'men;': 195, \"th'eyes\": 253, \"men's\": 194, 'lie.': 180, \"need'st\": 201, 'but': 5, 'mine': 197, 'flowery': 135, 'first;': 133, 'wake': 74, 'white,': 285, 'off': 207, 'so;': 59, 'doth': 120, 'hands,': 155, 'than': 65, 'must': 36, 'proud,': 50, 'angel,': 86, 'diadem': 117, 'off,': 208, 'soonest': 61, 'unpin': 274, 'joys.': 170, 'never': 202, 'us': 73, 'fate,': 22, 'will': 287, 'cast': 109, 'harmonious': 157, 'can': 7, 'coverings': 114, \"poppy'or\": 49, 'unbodied,': 271, \"swell'st\": 64, 'powers': 216, 'called': 6, 'stroke;': 63, 'grace': 149, 'then': 67, 'taste': 249, 'mystic': 198, 'received': 218, 'am': 83, 'it': 168, 'an': 85, 'flow,': 23, 'thee,': 257, 'die.': 16, 'have': 25, 'balls,': 90, 'made': 186, 'fairer': 130, 'new-found-land,': 203, 'happy': 156, 'no': 37, 'set,': 228, 'make': 28, 'nakedness!': 200, 'bed.': 94, 'tells': 251, 'walk': 280, 'how': 164, 'labour': 174, 'best': 2, 'fight.': 132, 'joys': 169, 'fools': 138, 'zone': 294, 'eternally,': 21, 'unclothed': 272, 'poor': 48, \"thou'art\": 70, 'used': 278, 'slave': 56, 'charms': 10, 'sickness': 55, 'may': 191, 'enter': 126, 'souls': 237, 'what': 282, 'evil': 128, 'hand': 154, 'poison,': 47, 'earthly': 122, 'death,': 12, 'such': 248, \"bring'st\": 105, 'envy,': 127, 'tired': 269, 'why': 78, 'shalt': 53, 'steals.': 244, 'short': 54, 'manned,': 190, 'meads': 193, 'thy': 72, 'whole': 286, 'covet': 115, 'well': 76, 'thou,': 265, 'dreadful,': 19, 'or': 212, 'linen': 183, 'sight,': 232, 'grow:': 150, 'busk,': 106, 'easily': 123, 'not,': 40, 'upright.': 276, 'having': 158, 'bodies': 100, 'oft-times': 209}\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we create a corpus object in sparse matrix format. Each row of the matrix corresponds to a vector that represents one of our documents. We serialize to disk to using the \"matrix market\" format. If you want to view the matrix as a list of lists, go ahead and comment out the third line in the cell below. Otherwise, when you print the corpus, you'll get a summary of the MmCorpus object that is created from '/tmp/johndonne.mm'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "corpora.MmCorpus.serialize('/tmp/johndonne.mm', corpus)\n",
      "corpus = corpora.MmCorpus('/tmp/johndonne.mm')\n",
      "print corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MmCorpus(2 documents, 295 features, 313 non-zero entries)\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A lot of natural language processing boils down to starting with a bag of words representation, converting to a different (sometimes more sophisticated) representation, and using the new representation to perform a classification task. In this case, we're going to use LSI (short for \"Latent Semantic Indexing\") to transform bags of words before comparing for similarity.\n",
      "\n",
      "At this point we're just transforming our representation of Donne's poetry. We'll get to comparison shortly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi = models.LsiModel(corpus, id2word = dictionary, num_topics = 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see what the two topics \"stand for\" by using the print_topic method. It looks like one of our topics is decent and the other ... not so much. We'll see that this is a problem later..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi.print_topics(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "['-0.210*\"like\" + -0.210*\"off\" + -0.172*\"be\" + -0.158*\"may\" + -0.112*\"be,\" + -0.112*\"can\" + -0.112*\"but\" + -0.112*\"though\" + -0.112*\"then\" + -0.112*\"must\"',\n",
        " '-0.291*\"death,\" + -0.194*\"much\" + -0.194*\"dost\" + -0.194*\"thy\" + -0.194*\"sleep\" + -0.181*\"rest\" + -0.181*\"not\" + -0.156*\"be\" + -0.097*\"best\" + -0.097*\"nor\"']"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, now it's time to introduce a new document. This is one of Andrew Marvell's poems. It should be more similar to Donne's Elegy XIX. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "to_his_coy_mistress = \"\"\"Had we but world enough, and time,\n",
      "This coyness, lady, were no crime.\n",
      "We would sit down and think which way\n",
      "To walk, and pass our long love's day;\n",
      "Thou by the Indian Ganges' side\n",
      "Shouldst rubies find; I by the tide\n",
      "Of Humber would complain. I would\n",
      "Love you ten years before the Flood;\n",
      "And you should, if you please, refuse\n",
      "Till the conversion of the Jews.\n",
      "My vegetable love should grow\n",
      "Vaster than empires, and more slow.\n",
      "An hundred years should go to praise\n",
      "Thine eyes, and on thy forehead gaze;\n",
      "Two hundred to adore each breast,\n",
      "But thirty thousand to the rest;\n",
      "An age at least to every part,\n",
      "And the last age should show your heart.\n",
      "For, lady, you deserve this state,\n",
      "Nor would I love at lower rate.\n",
      "\n",
      "        But at my back I always hear\n",
      "Time's winged chariot hurrying near;\n",
      "And yonder all before us lie\n",
      "Deserts of vast eternity.\n",
      "Thy beauty shall no more be found,\n",
      "Nor, in thy marble vault, shall sound\n",
      "My echoing song; then worms shall try\n",
      "That long preserv'd virginity,\n",
      "And your quaint honour turn to dust,\n",
      "And into ashes all my lust.\n",
      "The grave's a fine and private place,\n",
      "But none I think do there embrace.\n",
      "\n",
      "        Now therefore, while the youthful hue\n",
      "Sits on thy skin like morning dew,\n",
      "And while thy willing soul transpires\n",
      "At every pore with instant fires,\n",
      "Now let us sport us while we may;\n",
      "And now, like am'rous birds of prey,\n",
      "Rather at once our time devour,\n",
      "Than languish in his slow-chapp'd power.\n",
      "Let us roll all our strength, and all\n",
      "Our sweetness, up into one ball;\n",
      "And tear our pleasures with rough strife\n",
      "Thorough the iron gates of life.\n",
      "Thus, though we cannot make our sun\n",
      "Stand still, yet we will make him run.\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have the poem, we need to convert it to a format that we can use with our LSI model. We're going to get a bag of words and convert that to the lsi representation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec_bow = dictionary.doc2bow(to_his_coy_mistress.lower().split())\n",
      "vec_lsi = lsi[vec_bow]\n",
      "print vec_lsi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, -4.2406214701620897), (1, -3.2202687396148644)]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a sanity check, let's reproduce one of Donne's poems and use that for comparison against the corpus. We already know that there should be strong similarity between this poem and itself (as stored in the corpus object). If we don't get that, then we've done something horribly wrong..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "death_be_not_proud = \"\"\"Death, be not proud, though some have called thee Mighty and dreadful, \n",
      "for thou are not so; For those whom thou think'st thou dost overthrow Die not, \n",
      "poor Death, nor yet canst thou kill me. \n",
      "From rest and sleep, which but thy pictures be, \n",
      "Much pleasure; then from thee much more must flow, \n",
      "And soonest our best men with thee do go, Rest of their bones, \n",
      "and soul's delivery. Thou'art slave to fate, chance, kings, \n",
      "and desperate men, And dost with poison, war, and sickness dwell, \n",
      "And poppy'or charms can make us sleep as well And better than thy stroke; \n",
      "why swell'st thou then? One short sleep past, \n",
      "we wake eternally, And death shall be no more; \n",
      "Death, thou shalt die.\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we're simply converting from the string to a bag of words model to the LSI representation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "death_bow = dictionary.doc2bow(death_be_not_proud.lower().split())\n",
      "death_lsi = lsi[death_bow]\n",
      "print death_lsi"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, -2.4294372738830661), (1, -10.153710382528521)]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we create the structure that will allow us to perform similarity tests between our objects. We're going to use gensim's MatrixSimilarity object to compute similarities. This uses a dense numpy matrix that is stored in main memory. The similarity measure used is cosine similarity. That means that dissimilar vectors should be closer to -1, and similar vectors should be close to 1.\n",
      "\n",
      "Strictly speaking, we don't have to use the save and load functions here. We could just create the index object and use it directly. However, you'll often want to serialize to disk, and the cell below shows how you would do it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = similarities.MatrixSimilarity(lsi[corpus])\n",
      "index.save('/tmp/johndonne.index')\n",
      "index = similarities.MatrixSimilarity.load('/tmp/johndonne.index')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally! We can actually compute similarities. Recall from above that vec_lsi is the LSI representation of Marvell's poem. We want to compare that poem to each poem in our corpus and see how similar it is to each of our \"training\" examples. We see that it's similarish to both, which is better than being dissimilar to both, but not as good as being similar only to Elegy XIX..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sims = index[vec_lsi]\n",
      "print list(enumerate(sims))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0, 0.77349174), (1, 0.75181013)]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For control purposes, let's try a few other examples. First, an excerpt from the Wikipedia article on Andrew Marvell..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki_doc = \"\"\"Vincent Palmieri noted that Marvell is sometimes known as the \n",
      "\"British Aristides\" for his incorruptible integrity in life and poverty at \n",
      "death. Many of his poems were not published until 1681, three years after \n",
      "his death, from a collection owned by Mary Palmer, his housekeeper. After \n",
      "Marvell's death she laid dubious claim to having been his wife, from the \n",
      "time of a secret marriage in 1667.\"\"\"\n",
      "\n",
      "wiki_bow = dictionary.doc2bow(wiki_doc.lower().split())\n",
      "print wiki_bow\n",
      "\n",
      "wiki_lsi = lsi[wiki_bow]\n",
      "print wiki_lsi\n",
      "\n",
      "sims1 = index[wiki_lsi]\n",
      "print list(enumerate(sims1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(11, 1), (12, 1), (39, 1), (108, 1), (158, 1), (163, 5), (275, 1)]\n",
        "[(0, -0.56724030382162338), (1, -0.45519569581184038)]\n",
        "[(0, 0.79017746), (1, 0.73389357)]\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And then lastly, let's take the first few lines from Queen Mab, which has several references to Death. Hopefully this is similar to Donne's Death, Be Not Proud."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shelley = \"\"\"\n",
      "How wonderful is Death,\n",
      "Death, and his brother Sleep!\n",
      "One, pale as yonder waning moon\n",
      "With lips of lurid blue;\n",
      "The other, rosy as the morn\n",
      "When throned on ocean\u2019s wave\n",
      "It blushes o\u2019er the world;\n",
      "Yet both so passing wonderful! \n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shelley_bow = dictionary.doc2bow(shelley.lower().split())\n",
      "print shelley_bow\n",
      "shelley_lsi = lsi[shelley_bow]\n",
      "shelley_sims = index[shelley_lsi]\n",
      "print list(enumerate(shelley_sims))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(12, 2), (79, 1), (163, 1), (164, 1), (168, 1), (210, 1), (234, 1)]\n",
        "[(0, 0.95288152), (1, 0.45514712)]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see here that the program finds the \"right\" answer. That's a good thing?\n",
      "\n",
      "At the end of the day, we have a few problems:\n",
      "\n",
      "0. We don't have enough training examples. Using only two of Donne's poems pretty much ensures that we're going to get bad results. If we were really going to analyze poetry like this, we would want to use lots of examples to adequately represent the range of Donne's work. This is a problem we can solve just by getting more data. It might be for this particular task that all we really need is more data.\n",
      "\n",
      "1. Our preprocessing is bad. We don't handle punctuation, which a decent nlp system should. So \"death,\" and \"death\" become different words in the eyes of our system. This is an easy fix, and would handle some of the issues above.\n",
      "\n",
      "2. Our choice of representation is not adequate. The argument we're going to make in this course is that representing words and documents via ints and sparse vectors is too fragile (we're really just doing clever boolean comparisons). Using tricks like LSI will definitely help, but there's a lot of semantic information that we miss out on by using sparse vector representations. The goal of this course will be to see how we can use dense vector represenations to capture more of that semantic information, and perform interesting linguistic tasks."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}